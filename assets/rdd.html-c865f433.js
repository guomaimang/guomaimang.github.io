import{_ as i}from"./plugin-vue_export-helper-c27b6911.js";import{o as a,c as n,e as t}from"./app-454f912f.js";const e={},p=t('<h1 id="spark-rdd" tabindex="-1"><a class="header-anchor" href="#spark-rdd" aria-hidden="true">#</a> Spark RDD</h1><p>Resilient Distributed Datasets (RDD)</p><p>Spark RDD 在逻辑上被划分为多个节点，以便在集群的不同节点上并行计算。</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/08/d4fc3a18a53d0.png" alt="1715145643268.png" tabindex="0" loading="lazy"><figcaption>1715145643268.png</figcaption></figure><p>RDD: a <strong>distributed</strong> data structure !</p><ul><li>RDDs are divided into smaller chunks called partitions「分区」 <ul><li>The maximum size of a partition 2GB</li></ul></li><li>Partitions of an RDD are <strong>distributed across all the nodes</strong> in the cluster.</li><li>Two types of operations that you can perform on an RDD: <strong>Transformations</strong> and <strong>Actions</strong></li><li>RDD 是 &quot;不可变「immutable」 &quot;的（即创建后不可修改）-&gt; 当您可以通过转换修改 RDD 时，转换会返回一个新的 RDD，而原始 RDD 保持不变</li></ul><h2 id="rdd-internal" tabindex="-1"><a class="header-anchor" href="#rdd-internal" aria-hidden="true">#</a> RDD Internal</h2><p>Five main properties:</p><ul><li>An RDD contains list of <strong>partitions</strong>!</li><li>function to compute (e.g.,map,flatmap,..)</li><li>list of [parent RDD,type (wide/narrow)]</li><li>可选的分区方案（如 HashPartitioner、RangePartitioner）</li><li>Optionally, a computation placement hint <ul><li>用于计算每个分区的首选位置列表（例如 HDFS 文件的块位置）</li></ul></li></ul><img src="https://pic.hanjiaming.com.cn/2024/05/08/dfeaffa90bafa.png" alt="1715156611375.png" style="zoom:50%;"><h2 id="transformations-and-actions" tabindex="-1"><a class="header-anchor" href="#transformations-and-actions" aria-hidden="true">#</a> Transformations and Actions</h2><p>RDD keeps a pointer to it&#39;s parent RDD(s)</p><ul><li>Transformations：在 RDD 上应用某些函数并创建一个新的 RDD。它不会修改您应用该函数的 RDD。</li><li>Action 用于将结果保存到某个位置（如 HDFS、S3）或通过驱动程序显示结果。</li></ul><figure><img src="https://pic.hanjiaming.com.cn/2024/05/08/afb5e93275308.png" alt="1715156810288.png" tabindex="0" loading="lazy"><figcaption>1715156810288.png</figcaption></figure><ul><li>Transformations: 每种转换都接收（一个或多个）RDD，并通过对内容应用某些函数（如 map、filter、flatMap、union、groupBy、reduceByKey、sortByKey、join）输出转换后的 RDD（另一个 RDD</li><li>Actions: 在数据集上运行计算后，将值返回给驱动程序「driver program」（reduce、collect、count、take、countByKey）、saveAsTextFile、saveAsObjectFile，...</li></ul><p>“Spark 中的所有转换都是惰性的”：仅当 Action 需要将结果返回给驱动程序时才计算 RDD（更多内容待讨论）</p><div class="hint-container note"><p class="hint-container-title">Example: “Transformation returns you a new RDD”</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/08/cb7c570fe73da.png" alt="1715157007979.png" tabindex="0" loading="lazy"><figcaption>1715157007979.png</figcaption></figure></div><figure><img src="https://pic.hanjiaming.com.cn/2024/05/08/4bef5670ec546.png" alt="1715157196375.png" tabindex="0" loading="lazy"><figcaption>1715157196375.png</figcaption></figure><h3 id="rdd-creation" tabindex="-1"><a class="header-anchor" href="#rdd-creation" aria-hidden="true">#</a> RDD Creation</h3><p>Spark 支持文本文件、序列文件和其他任何 Hadoop 输入格式。</p><p>默认情况下，Spark 会为 HDFS 中的每个文件块创建一个分区（例如，默认为 64MB/128MB）。</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/08/e31b384418441.png" alt="1715157481983.png" tabindex="0" loading="lazy"><figcaption>1715157481983.png</figcaption></figure><h3 id="map-func" tabindex="-1"><a class="header-anchor" href="#map-func" aria-hidden="true">#</a> map(func)</h3><p>在每条记录上执行一个函数（类似于 Hadoop 中的 Map）</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/08/d5015cca21f6b.png" alt="1715157581037.png" tabindex="0" loading="lazy"><figcaption>1715157581037.png</figcaption></figure><h3 id="flatmap-func" tabindex="-1"><a class="header-anchor" href="#flatmap-func" aria-hidden="true">#</a> flatMap(func)</h3><figure><img src="https://pic.hanjiaming.com.cn/2024/05/08/27696cc63ff1f.png" alt="1715158200190.png" tabindex="0" loading="lazy"><figcaption>1715158200190.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/08/e71515125fc53.png" alt="1715158286744.png" tabindex="0" loading="lazy"><figcaption>1715158286744.png</figcaption></figure><div class="hint-container info"><p class="hint-container-title">Examples Transformation</p><ul><li>map(func)：返回一个新的分布式数据集，该数据集是通过函数 func 传递源的每个元素而形成的。</li><li>filter(function)：返回一个新的数据集，该数据集是通过选择源中函数返回 true 的那些元素而形成的。</li><li>flatMap(func)：与 map 类似，但每个输入项可以映射到 0 个或多个输出项。</li><li>union(otherDataset):返回一个新数据集，其中包含源数据集和参数</li><li>groupByKey([numTasks]):当在(K,V)对数据集上调用时，返回一个(K,Iterable <code>&lt;V&gt;</code>)对数据集。</li><li>join (otherDataset,[numTasks]):当在 (K,V) 和 (K,W) 类型的数据集上调用时，返回一个 (K,<code>(V,W)</code>) 成对的数据集，其中包含每个键的所有元素对。</li></ul></div><h3 id="reduce-func" tabindex="-1"><a class="header-anchor" href="#reduce-func" aria-hidden="true">#</a> reduce(func)</h3><figure><img src="https://pic.hanjiaming.com.cn/2024/05/08/a8643eeca24af.png" alt="1715159027124.png" tabindex="0" loading="lazy"><figcaption>1715159027124.png</figcaption></figure><h2 id="examples-of-actions" tabindex="-1"><a class="header-anchor" href="#examples-of-actions" aria-hidden="true">#</a> Examples of Actions</h2><h3 id="collect" tabindex="-1"><a class="header-anchor" href="#collect" aria-hidden="true">#</a> collect()</h3><p>collect()：以数组形式向驱动程序返回数据集的所有元素。</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/09/db7203a3e1bdd.png" alt="1715268333809.png" tabindex="0" loading="lazy"><figcaption>1715268333809.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/09/e38beb43351a9.png" alt="1715268387186.png" tabindex="0" loading="lazy"><figcaption>1715268387186.png</figcaption></figure><h3 id="others" tabindex="-1"><a class="header-anchor" href="#others" aria-hidden="true">#</a> Others</h3><figure><img src="https://pic.hanjiaming.com.cn/2024/05/09/d1688a1c58355.png" alt="1715268406925.png" tabindex="0" loading="lazy"><figcaption>1715268406925.png</figcaption></figure><h2 id="transformations" tabindex="-1"><a class="header-anchor" href="#transformations" aria-hidden="true">#</a> Transformations</h2><ol><li>Narrow Transformation: <ul><li>all the elements that are required to compute the records in single partition live <u>in the single partition of parent RDD</u> &gt; No shuffling!</li></ul></li><li>Wide Transformation: <ul><li>all the elements that are required to compute the records in the single partition may live <strong>in many partitions</strong> of parent RDD &gt; Shuffle Needed</li></ul></li></ol><p>E.g., {aggregate/group/sort/ reduce} ByKey,intersect,join?, repartition,…</p><img src="https://pic.hanjiaming.com.cn/2024/05/09/0225f7c14e426.png" alt="1715268574420.png" style="zoom:75%;"><h3 id="narrow-transformation" tabindex="-1"><a class="header-anchor" href="#narrow-transformation" aria-hidden="true">#</a> Narrow Transformation</h3><p>父RDD的每个分区用于生成子RDD的一个分区。</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/09/2dbf099c5a09d.png" alt="1715268638209.png" tabindex="0" loading="lazy"><figcaption>1715268638209.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/09/9a01fa701cfba.png" alt="1715268694872.png" tabindex="0" loading="lazy"><figcaption>1715268694872.png</figcaption></figure><h3 id="wide-transformation" tabindex="-1"><a class="header-anchor" href="#wide-transformation" aria-hidden="true">#</a> Wide Transformation</h3><p>将单个父 RDD 分区中的记录分散（洗牌）到多个子 RDD 分区中（例如，groupByKey、reduceByKey）。</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/09/932d1d0624310.png" alt="1715268748453.png" tabindex="0" loading="lazy"><figcaption>1715268748453.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/09/d948d5246131e.png" alt="1715268770697.png" tabindex="0" loading="lazy"><figcaption>1715268770697.png</figcaption></figure><p>注意：reduceByKey 在 shuffle 之前聚合 key 减少网络流量（更高效）</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/09/96401867106e3.png" alt="1715268863101.png" tabindex="0" loading="lazy"><figcaption>1715268863101.png</figcaption></figure><p>通过获取源 RDD 的所有不同元素返回 RDD--需要跨分区洗牌。</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/09/1e1f76210d0e8.png" alt="1715268980123.png" tabindex="0" loading="lazy"><figcaption>1715268980123.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/9b6ab7e57ad72.png" alt="1715276653815.png" tabindex="0" loading="lazy"><figcaption>1715276653815.png</figcaption></figure><div class="hint-container note"><p class="hint-container-title">Special Note on Join()</p><p>“共同分区”：rdd1和rdd2 使用相同的分区器（HashPartitioner）并且也具有相同数量的分区 -&gt; 不需要shuffle</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/f6c6456957700.png" alt="1715276936461.png" tabindex="0" loading="lazy"><figcaption>1715276936461.png</figcaption></figure></div><h3 id="shuffle" tabindex="-1"><a class="header-anchor" href="#shuffle" aria-hidden="true">#</a> Shuffle</h3><p>Shuffle is an expensive operation</p><p>(Shuffle Write Shuffle Read)</p><p>基本上Spark的工作原理与Hadoop相同——执行映射端排序并写入本地磁盘（shuffleWrite）。接下来阶段的任务通过网络获取分区（shuffleRead）。</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/add80c2873452.png" alt="1715277101977.png" tabindex="0" loading="lazy"><figcaption>1715277101977.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/c3c7abc7e7c99.png" alt="1715277131111.png" tabindex="0" loading="lazy"><figcaption>1715277131111.png</figcaption></figure><h2 id="rdd-partitions" tabindex="-1"><a class="header-anchor" href="#rdd-partitions" aria-hidden="true">#</a> RDD Partitions</h2><p>默认情况下，每个 HDFS 分区都会创建一个分区，大小为 64MB/128MB；或您正在读取的文件数。</p><p>Performance sensitive</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/7473072939afe.png" alt="1715277491332.png" tabindex="0" loading="lazy"><figcaption>1715277491332.png</figcaption></figure><ul><li>分区太少 -&gt; 无法利用集群中所有可用的核心。</li><li>分区太多 -&gt; 管理许多小任务的开销太大。</li></ul><h2 id="spark-execution-flow" tabindex="-1"><a class="header-anchor" href="#spark-execution-flow" aria-hidden="true">#</a> Spark Execution Flow</h2><p>&quot;All transformations in Spark are lazy&quot;: Spark 在执行第一个 &quot;操作&quot;（如 reduce()or collect()）之前不会计算任何内容，此时它会递归计算所有 RDDs</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/497e50cfe1895.png" alt="1715280723639.png" tabindex="0" loading="lazy"><figcaption>1715280723639.png</figcaption></figure><p>用户应用程序创建 RDDs、转换 RDDs 并运行操作，从而形成一个由操作符组成的 DAG（有向无环图）。</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/a466d72b30e5b.png" alt="1715280784257.png" tabindex="0" loading="lazy"><figcaption>1715280784257.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/4842df1911423.png" alt="1715280797311.png" tabindex="0" loading="lazy"><figcaption>1715280797311.png</figcaption></figure><details class="hint-container details"><summary>详情</summary><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/8ef393f1b0c54.png" alt="1715280831759.png" tabindex="0" loading="lazy"><figcaption>1715280831759.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/701423eb6e40f.png" alt="1715280850906.png" tabindex="0" loading="lazy"><figcaption>1715280850906.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/8a873346eda05.png" alt="1715280863692.png" tabindex="0" loading="lazy"><figcaption>1715280863692.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/140a6094c89aa.png" alt="1715280922036.png" tabindex="0" loading="lazy"><figcaption>1715280922036.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/c089a52efe7a8.png" alt="1715280938881.png" tabindex="0" loading="lazy"><figcaption>1715280938881.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/3fd601e8f9257.png" alt="1715281004501.png" tabindex="0" loading="lazy"><figcaption>1715281004501.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/b3d13fcac50cd.png" alt="1715281057561.png" tabindex="0" loading="lazy"><figcaption>1715281057561.png</figcaption></figure><p>Schedule and Execute “stage by stage” (DAGScheduler)</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/24c5f172c9881.png" alt="1715281104274.png" tabindex="0" loading="lazy"><figcaption>1715281104274.png</figcaption></figure><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/5d0fd6495c4a9.png" alt="1715281213589.png" tabindex="0" loading="lazy"><figcaption>1715281213589.png</figcaption></figure></details><p>“Spark is lazy”: Advantages:</p><ul><li>Opportunities of optimization <ul><li>在有机会查看整个 DAG 之后，再做出优化决策（执行计划）。</li><li>Narrow transformations 可以捆绑在一起，以实现内存计算（流水线）。</li></ul></li><li>Saves computation and increases speed <ul><li>只计算 RDD，生成 Action 想要的值。</li><li>基于数据位置的动态任务分配（在缓存了 RDD 分区的节点上运行任务）</li></ul></li></ul><h2 id="rdd-cache" tabindex="-1"><a class="header-anchor" href="#rdd-cache" aria-hidden="true">#</a> RDD Cache</h2><p>In-memory Computing: RDD Cache</p><p>RDD 可缓存在 RAM 中，以实现快速访问重用</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/0b2197a9a4d66.png" alt="1715281443519.png" tabindex="0" loading="lazy"><figcaption>1715281443519.png</figcaption></figure><p>How to cache an RDD?</p><ul><li>只需在驱动程序中执行 rdd.cache()，其中 rdd 是用户可访问的 RDD</li><li>In Spark shell:scala&gt;RDDName.cache()</li><li>Executor 将 RDD 分区作为 Java 对象缓存在 Java 堆中。</li></ul><p>请记住，执行器内存是有限的。Spark 会以最近最少使用（LRU）的方式自动从执行器内存中驱逐 RDD 分区。</p><p>缓存 RDD 时，它的分区会被加载到持有它的执行器的内存中。</p><figure><img src="https://pic.hanjiaming.com.cn/2024/05/10/8d547707e22ea.png" alt="1715281550272.png" tabindex="0" loading="lazy"><figcaption>1715281550272.png</figcaption></figure><ul><li>可以增加 &quot;缓存的分数&quot;，因为重新分区后，分区的大小会变小，更容易装入内存。</li><li>Advice:&quot;Avoid shuffle&quot;unless needed! <ul><li>注意：虽然缓存提高了速度，但 Spark 会在每次洗牌时将分区写入磁盘（洗牌写入）（类似于 MapReduce）。</li></ul></li></ul><p>“缓存只是一个提示，而不是保证”。</p><ul><li>Spark 监控每个节点上的缓存使用情况，并以 LRU 方式丢弃旧数据分区。</li><li>内存中无法容纳的缓存分区会在需要时溢出到磁盘或在运行中重新计算，这取决于 RDD 的存储级别（在第二部分中解释）。</li></ul><p>如果 RDD 没有缓存，一旦被消耗（例如，在操作之后），就会被垃圾回收，需要重新计算！</p><p>When to Use cache()?</p><ul><li>Use an RDD many times</li><li>Performing multiple actions on the same RDD</li><li>For long chains of (or very expensive) transformations</li></ul><p>Limitation of caching:</p><ul><li>缓存减少了执行引擎本身的可用内存（第 4 讲第 Il 部分）</li><li>缓存的分区可能会在实际重新使用之前被逐出</li></ul>',93),c=[p];function r(g,o){return a(),n("div",null,c)}const f=i(e,[["render",r],["__file","rdd.html.vue"]]);export{f as default};
