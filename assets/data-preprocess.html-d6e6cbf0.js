import{_ as n}from"./plugin-vue_export-helper-c27b6911.js";import{r as t,o as l,c as r,a as o,b as i,d as s,w as d,e as a}from"./app-4a3e2d8d.js";const c={},u=a('<h1 id="data-preprocessing" tabindex="-1"><a class="header-anchor" href="#data-preprocessing" aria-hidden="true">#</a> Data Preprocessing</h1><h2 id="major-tasks" tabindex="-1"><a class="header-anchor" href="#major-tasks" aria-hidden="true">#</a> Major Tasks</h2><ul><li>Data cleaning: 填补缺失值，平滑噪声数据，识别或删除异常值，解决不一致问题</li><li>Data integration: 整合多个数据库、数据集或文件</li><li>Data transformation: Normalization and aggregation</li><li>Data reduction: 在体积上获得较少的代表性，但产生相同或相似的分析结果</li><li>Discretization「离散化」 and concept hierarchy generation <ul><li>数据缩减的一部分，但特别重要，尤其是对数值数据而言</li></ul></li></ul><h2 id="why-preprocess-data" tabindex="-1"><a class="header-anchor" href="#why-preprocess-data" aria-hidden="true">#</a> Why Preprocess Data</h2><p>Data in the real world is dirty</p><ul><li>incomplete：缺少属性值，缺少某些感兴趣的属性，或仅包含汇总数据</li><li>noisy: containing errors or outliers</li><li>inconsistent: containing discrepancies「差异」 in codes or names</li></ul><p>No quality data, no quality mining results!</p><ul><li>优质决策必须以优质数据为基础</li><li>Data warehouse needs consistent integration of quality data</li></ul><div class="hint-container tip"><p class="hint-container-title">Data Quality</p><p>Multi-Dimensional Measure of Data Quality</p><p>一种广为接受的多维观点：</p><ul><li>Accuracy</li><li>Completeness</li><li>Consistency</li><li>Timeliness「时效性」</li><li>Believability「可信性」</li><li>Value added</li><li>Interpretability「可解释性」</li><li>Accessibility「可访问性」</li></ul></div><h2 id="data-cleaning" tabindex="-1"><a class="header-anchor" href="#data-cleaning" aria-hidden="true">#</a> Data Cleaning</h2><h3 id="missing-data" tabindex="-1"><a class="header-anchor" href="#missing-data" aria-hidden="true">#</a> Missing Data</h3>',11),h=a('<h3 id="noisy-data" tabindex="-1"><a class="header-anchor" href="#noisy-data" aria-hidden="true">#</a> Noisy Data</h3><p>Noise: <u>random error</u> or <u>variance</u> in a measured variable</p><p>Incorrect attribute values may due to</p><ul><li>错误的数据收集工具</li><li>data entry problems</li><li>data transmission problems</li><li>technology limitation</li><li>inconsistency in naming convention</li></ul><p>Other data problems which requires data cleaning</p><ul><li>duplicate records</li><li>incomplete data「数据不完整」</li><li>inconsistent data「数据不一致」</li></ul><div class="hint-container info"><p class="hint-container-title">How to Handle Noisy Data</p><ul><li>Binning method「分箱法」: <ul><li>first sort data and partition into (equi-depth) bins「（等深）分区」</li><li>然后可以通过 bin 的均值、bin 的中值、bin 的边界等来平滑 bin 中的数据。</li></ul></li><li>Clustering：detect and remove outliers</li><li>Combined computer and human inspection: 检测可疑值并由人工检查</li><li>Regression: smooth by fitting the data into regression functions</li></ul></div><h4 id="binning" tabindex="-1"><a class="header-anchor" href="#binning" aria-hidden="true">#</a> Binning</h4><p>这是一种简单的离散化方法</p><ul><li>Equal-width (distance) partitioning <ul><li>它将范围划分为 N 个大小相等的区间：均匀网格</li><li>如果 A 和 B 是属性的最低值和最高值，则区间宽度为W = (B-A)/N。</li><li>最直接</li><li>但离群值可能会在演示中占主导地位</li><li>偏斜数据「Skewed data」不好处理</li></ul></li><li>Equal-depth (frequency) partitioning: <ul><li>它将范围划分为 N 个区间，每个区间包含大致相同数量的样本</li><li>良好的数据缩放</li><li>管理分类属性可能很棘手</li></ul></li></ul><h4 id="binning-methods-for-data-smoothing" tabindex="-1"><a class="header-anchor" href="#binning-methods-for-data-smoothing" aria-hidden="true">#</a> Binning Methods for Data Smoothing</h4><img src="https://pic.hanjiaming.com.cn/2023/12/13/e33e126301195.png" alt="1702407417961.png" style="zoom:33%;"><h4 id="smoothing-by-cluster-analysis" tabindex="-1"><a class="header-anchor" href="#smoothing-by-cluster-analysis" aria-hidden="true">#</a> Smoothing by Cluster Analysis</h4><img src="https://pic.hanjiaming.com.cn/2023/12/13/3f1988563cb31.png" alt="1702407463425.png" style="zoom:50%;"><h4 id="smoothing-by-regression" tabindex="-1"><a class="header-anchor" href="#smoothing-by-regression" aria-hidden="true">#</a> Smoothing by Regression</h4><img src="https://pic.hanjiaming.com.cn/2023/12/13/b4ddc34b58920.png" alt="1702407514519.png" style="zoom:33%;"><h2 id="data-integration" tabindex="-1"><a class="header-anchor" href="#data-integration" aria-hidden="true">#</a> Data Integration</h2><p>Handling <u>Redundant Data</u> in Data Integration</p><ul><li>Redundant data occur often when integrating multiple databases <ul><li>相同的属性在不同的数据库中可能有不同的名称</li><li>一个属性可能是另一个表中的“派生”属性，例如年收入</li></ul></li><li>通过相关分析可以检测到冗余数据</li><li>仔细整合多个来源的数据，有助于减少/避免冗余和不一致，提高采矿速度和质量</li></ul><h2 id="data-transformation" tabindex="-1"><a class="header-anchor" href="#data-transformation" aria-hidden="true">#</a> Data Transformation</h2><p>Different Forms of Transformation</p><ul><li>Smoothing (is a form of transformation): remove noise from data</li><li>Aggregation: summarization, data cube construction</li><li>Generalization「泛化」: concept hierarchy climbing</li><li>Normalization: scaled to fall within a small, specified range <ul><li>通过十进制缩放标准化</li><li>min-max normalization</li><li>z-score normalization</li></ul></li><li>Attribute/feature construction：从给定的属性构造新的属性</li></ul><h2 id="data-reduction" tabindex="-1"><a class="header-anchor" href="#data-reduction" aria-hidden="true">#</a> Data Reduction</h2><p>仓库可存储 TB 级的数据：复杂的数据分析/挖掘可能需要很长时间才能在完整的数据集上运行</p><p>Data reduction: 获得数据集的缩减表示，体积更小，但能产生相同（或几乎相同）的分析结果</p><p>Data reduction strategies</p><ul><li><strong>Dimensionality reduction</strong></li><li>Numerosity reduction</li><li>Discretization and concept hierarchy generation「离散化和概念层次结构生成」</li></ul><h3 id="dimensionality-reduction" tabindex="-1"><a class="header-anchor" href="#dimensionality-reduction" aria-hidden="true">#</a> Dimensionality Reduction</h3><p>方法1：Feature selection (i.e., attribute subset selection):</p><ul><li>选择最小的特征集，使给定这些特征值的不同类别的概率分布尽可能接近给定所有特征值的原始分布</li><li>减少模式中的模式数量，更容易理解</li></ul><img src="https://pic.hanjiaming.com.cn/2023/12/14/d477cbfcc48e1.png" alt="1702496493439.png" style="zoom:50%;"><p>方法2：特征提取/转换 - 将现有特征组合（映射）为较少数量的新/替代特征</p><ul><li>线性组合（投影「projection」）</li><li>Nonlinear combination</li></ul><div class="hint-container tip"><p class="hint-container-title">Feature Selection vs Extraction</p><ul><li>特征选择「Feature selection」：选择k&lt;d个重要特征，忽略剩余的d-k <ul><li>子集选择算法</li></ul></li><li>特征提取「Feature extraction」：将原始 xi , i =1,...,d 维度投影到新的 k&lt;d 维度，zj , j =1,...,k <ul><li>主成分分析（PCA）、线性判别分析（LDA）、因子分析（FA）</li></ul></li></ul></div><h4 id="subset-selection" tabindex="-1"><a class="header-anchor" href="#subset-selection" aria-hidden="true">#</a> Subset Selection</h4><img src="https://pic.hanjiaming.com.cn/2023/12/14/aee6f8b61106d.png" alt="1702497345079.png" style="zoom:33%;"><h4 id="linear-dimensionality-reduction" tabindex="-1"><a class="header-anchor" href="#linear-dimensionality-reduction" aria-hidden="true">#</a> Linear dimensionality reduction</h4><p>将 n 维数据线性投影到 k 维空间</p><img src="https://pic.hanjiaming.com.cn/2023/12/14/64cd2359fbf88.png" alt="1702497526463.png" style="zoom:33%;"><p>用于投影的最佳 k 维子空间取决于任务</p><ul><li>分类：最大化类之间的分离（如 SVM） <ul><li>Example: linear discriminant analysis (LDA)</li><li>Dimensionality reduction is not limited to unsupervised learning!</li></ul></li><li>回归：最大化预测数据和响应变量之间的相关性 <ul><li>Example: partial least squares (PLS)</li></ul></li><li>Unsupervised: retain as much data variance as possible <ul><li>Example: principal component analysis (PCA)</li></ul></li></ul><h4 id="decision-tree-induction-for-dr" tabindex="-1"><a class="header-anchor" href="#decision-tree-induction-for-dr" aria-hidden="true">#</a> Decision Tree Induction for DR</h4><img src="https://pic.hanjiaming.com.cn/2023/12/14/6a01b35db9807.png" alt="1702497729212.png" style="zoom:33%;"><h3 id="numerosity-reduction" tabindex="-1"><a class="header-anchor" href="#numerosity-reduction" aria-hidden="true">#</a> Numerosity Reduction</h3><blockquote><p>“我们可以通过选择替代的、‘较小’的数据表示形式来减少数据量吗？”</p></blockquote><p>Parametric methods「参数方法」</p><ul><li>假设数据符合某个（回归）模型，估计模型参数，只存储参数，丢弃数据（可能的异常值除外）</li><li>E.g. Linear regression where data are modeled to fit a straight line</li></ul><p>Non-parametric methods</p><ul><li>Do not assume models</li><li>Major families: histograms, clustering, sampling</li></ul><h4 id="non-parametric-numerosity-reduction-histograms" tabindex="-1"><a class="header-anchor" href="#non-parametric-numerosity-reduction-histograms" aria-hidden="true">#</a> Non-parametric Numerosity Reduction: Histograms</h4><ul><li>流行的数据缩减技术</li><li>将这些数字分成若干个桶，并存储每个桶的平均值（总和）（以表示所有这些原始数字）</li><li>可通过动态编程在一维范围内进行优化构建</li><li>与量化问题有关。</li></ul><img src="https://pic.hanjiaming.com.cn/2023/12/14/ae0e610acb4b5.png" alt="1702498020869.png" style="zoom:33%;"><h4 id="data-reduction-by-clustering" tabindex="-1"><a class="header-anchor" href="#data-reduction-by-clustering" aria-hidden="true">#</a> Data Reduction By Clustering</h4><ul><li><p>将数据集划分为聚类，并且只能存储聚类表示法</p></li><li><p>如果数据是聚类的，则非常有效，但如果数据是 &quot;模糊 &quot;的，则无效</p></li><li><p>可以进行分层聚类，并存储在多维索引树结构中</p></li><li><p>聚类定义和聚类算法有多种选择</p></li></ul><h4 id="data-reduction-by-sampling" tabindex="-1"><a class="header-anchor" href="#data-reduction-by-sampling" aria-hidden="true">#</a> Data Reduction by Sampling</h4><ul><li>允许挖掘算法以与数据大小可能呈次线性关系的复杂性运行</li><li>选择具有代表性的数据子集 <ul><li><u>在存在偏斜的情况下，简单随机抽样的性能可能很差</u></li></ul></li><li>开发适应性取样方法</li><li>分层抽样： <ul><li>每个类别（或相关亚群）在整个数据库中的大致百分比</li><li>与偏斜数据结合使用</li></ul></li></ul><img src="https://pic.hanjiaming.com.cn/2023/12/14/225a2ddfdda45.png" alt="1702498190922.png" style="zoom:25%;"><h2 id="discretization" tabindex="-1"><a class="header-anchor" href="#discretization" aria-hidden="true">#</a> Discretization</h2><p>Three types of attributes:</p><ul><li>Nominal/Discrete/Categorical — values from an unordered set (e.g. color set)</li><li>Ordinal — 有序集合中的值（5 分制的 1,2,3,4,5，如电影评级）</li><li>Continuous — real numbers</li></ul><p>Discretization:</p><ul><li>将连续属性的范围划分为区间，例如使用分箱方法</li><li>有些分类算法只接受分类属性</li><li>通过离散化减少数据量</li><li>为进一步分析做好准备</li></ul><div class="hint-container info"><p class="hint-container-title">Discretization and Concept Hierarchy</p><p>Discretization: 通过将属性范围划分为区间，减少给定连续属性的数值数量。然后就可以用区间标签来替代实际数据值。</p><p>Concept hierarchies:</p><ul><li>通过收集低级概念（如年龄属性的数值）并将其替换为高级概念（如青年、中年或老年）来减少数据。</li><li>其他示例包括构建→街道→地区→城市→国家以及用于广义关联规则挖掘的层次结构。</li></ul></div>',63);function p(m,g){const e=t("RouterLink");return l(),r("div",null,[u,o("p",null,[i("Recall the "),s(e,{to:"/note/ds/da/data-mining/feature-engineering.html#how-to-handle-missing-data"},{default:d(()=>[i("feature engineering")]),_:1})]),h])}const y=n(c,[["render",p],["__file","data-preprocess.html.vue"]]);export{y as default};
