---
article: false
date: 2022-07-18
order: 5
author: Hirsun, Belter
headerDepth: 2
---

# Logistic Regression Model

æœºå™¨å­¦ä¹ å¾ˆåƒæ˜¯ä¸€ä¸ªæ¨¡å‹å¯¹å¤–ç•Œçš„åˆºæ¿€ï¼ˆè®­ç»ƒæ ·æœ¬ï¼‰åšå‡ºååº”ï¼Œè¶‹åˆ©é¿å®³ï¼ˆè¯„ä»·æ ‡å‡†ï¼‰ã€‚

## Classification

- Binary Classification
- Multi-class Classification

**é€»è¾‘å›å½’å¯ä»¥ç”¨æ¥è§£å†³äºŒåˆ†ç±»é—®é¢˜**ã€‚å¯ä»¥æ‹“å±•åˆ°å¤šåˆ†ç±»é—®é¢˜ã€‚

::: tip

- å›å½’æ¨¡å‹çš„è¾“å‡ºè¿ç»­çš„ï¼Œåˆ†ç±»æ¨¡å‹çš„è¾“å‡ºæ˜¯è¿ç»­çš„ã€‚
- ä¸¥è°¨çš„è¯´ï¼Œé€»è¾‘å›å½’ä¸ç®—æ˜¯å›å½’æ¨¡å‹ã€‚

:::

## ä»€ä¹ˆæ˜¯é€»è¾‘å›å½’

**é€»è¾‘å›å½’ = çº¿æ€§å›å½’ + sigmoidå‡½æ•°**

ä»å¤§çš„ç±»åˆ«ä¸Šæ¥è¯´ï¼Œé€»è¾‘å›å½’æ˜¯ä¸€ç§æœ‰**ç›‘ç£**çš„ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ï¼Œä¸»è¦ç”¨äºå¯¹æ ·æœ¬è¿›è¡Œ**åˆ†ç±»**ã€‚

åœ¨çº¿æ€§å›å½’æ¨¡å‹ä¸­ï¼Œè¾“å‡ºä¸€èˆ¬æ˜¯è¿ç»­çš„ï¼Œä¾‹å¦‚ $y=f(x)=a x+b$ ï¼Œå¯¹äºæ¯ä¸€ä¸ªè¾“å…¥çš„ xï¼Œéƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„ y è¾“å‡ºã€‚æ¨¡å‹çš„å®šä¹‰åŸŸå’Œå€¼åŸŸéƒ½å¯ä»¥æ˜¯ [-âˆ, +âˆ] ã€‚

ä½†æ˜¯å¯¹äºé€»è¾‘å›å½’ï¼Œè¾“å…¥å¯ä»¥æ˜¯è¿ç»­çš„ [-âˆ, +âˆ] ï¼Œä½†è¾“å‡ºä¸€èˆ¬æ˜¯ç¦»æ•£çš„ï¼Œå³åªæœ‰æœ‰é™å¤šä¸ªè¾“å‡ºå€¼ã€‚ä¾‹å¦‚ï¼Œå…¶å€¼åŸŸå¯ä»¥åªæœ‰ä¸¤ä¸ªå€¼ {0, 1} ï¼Œè¿™ä¸¤ä¸ªå€¼å¯ä»¥è¡¨ç¤ºå¯¹æ ·æœ¬çš„æŸç§åˆ†ç±»ï¼Œé«˜/ä½ã€æ‚£ç—…/å¥åº·ã€é˜´æ€§/é˜³æ€§ç­‰ï¼Œè¿™å°±æ˜¯æœ€å¸¸è§çš„äºŒåˆ†ç±»é€»è¾‘å›å½’ã€‚

![1676958067849.png](https://pic.hanjiaming.com.cn/2023/02/21/2da4df0fa888c.png)

å› æ­¤ï¼Œä»æ•´ä½“ä¸Šæ¥è¯´ï¼Œ **é€šè¿‡é€»è¾‘å›å½’æ¨¡å‹ï¼Œæˆ‘ä»¬å°†åœ¨æ•´ä¸ªå®æ•°èŒƒå›´ä¸Šçš„xæ˜ å°„åˆ°äº†æœ‰é™ä¸ªç‚¹ä¸Šï¼Œè¿™æ ·å°±å®ç°äº†å¯¹xçš„åˆ†ç±»ã€‚** å› ä¸ºæ¯æ¬¡æ‹¿è¿‡æ¥ä¸€ä¸ªxï¼Œç»è¿‡é€»è¾‘å›å½’åˆ†æï¼Œå°±å¯ä»¥å°†å®ƒå½’å…¥æŸä¸€ç±»yä¸­ã€‚

## é€»è¾‘å›å½’ä¸çº¿æ€§å›å½’çš„å…³ç³»

é€»è¾‘å›å½’ä¹Ÿè¢«ç§°ä¸º**å¹¿ä¹‰çº¿æ€§å›å½’æ¨¡å‹**ï¼Œå®ƒä¸çº¿æ€§å›å½’æ¨¡å‹çš„å½¢å¼åŸºæœ¬ä¸Šç›¸åŒï¼Œéƒ½å…·æœ‰ ax+b ï¼Œå…¶ä¸­ a å’Œ b æ˜¯å¾…æ±‚å‚æ•°ï¼Œå…¶åŒºåˆ«åœ¨äºä»–ä»¬çš„å› å˜é‡ä¸åŒ

- å¤šé‡çº¿æ€§å›å½’ç›´æ¥å°† ax+b ä½œä¸ºå› å˜é‡ï¼Œå³ y = ax+b 
- logistic å›å½’ åˆ™é€šè¿‡ å‡½æ•°S å°† ax+b å¯¹åº”åˆ°ä¸€ä¸ªéšçŠ¶æ€ pï¼Œ**p = S(ax+b)**ï¼Œ**ç„¶åæ ¹æ® p ä¸ 1-p çš„å¤§å°å†³å®šå› å˜é‡çš„å€¼**ã€‚è¿™é‡Œçš„å‡½æ•°Så°±æ˜¯Sigmoidå‡½æ•°ã€‚

$$
S(t)=\frac{1}{1+e^{-t}}
$$

å°† t æ¢æˆ ax+bï¼Œå¯ä»¥å¾—åˆ°é€»è¾‘å›å½’æ¨¡å‹çš„å‚æ•°å½¢å¼ï¼š
$$
p(x ; a, b)=\frac{1}{1+e^{-(a x+b)}}
$$
é€šç”¨å½¢å¼æ˜¯ $h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T} x}}$

::: tip sigmoidå‡½æ•°çš„å›¾åƒ

$S(x)=\frac{1}{1+e^{-x}}$

<img src="https://pic.hanjiaming.com.cn/2022/07/10/b0dc43dd4f7b7.png" alt="1657467796254.png" style="zoom: 50%;" />

:::

é€šè¿‡ å‡½æ•°S çš„ä½œç”¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¾“å‡ºçš„å€¼é™åˆ¶åœ¨åŒºé—´ (0ï¼Œ1)  ä¸Šã€‚p(x )åˆ™å¯ä»¥ç”¨æ¥è¡¨ç¤ºæ¦‚ç‡ p(y=1|x)ï¼Œå³å½“ä¸€ä¸ª x å‘ç”Ÿæ—¶ï¼Œy è¢«åˆ†åˆ°1é‚£ä¸€ç»„çš„æ¦‚ç‡ã€‚

å…¶å®åœ¨çœŸå®æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°çš„yçš„å€¼æ˜¯åœ¨ [0, 1] è¿™ä¸ªåŒºé—´ä¸Šçš„ä¸€ä¸ªæ•°ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸€ä¸ªé˜ˆå€¼ï¼Œé€šå¸¸æ˜¯ 0.5ï¼Œå½“ y>0.5 æ—¶ï¼Œå°±å°†è¿™ä¸ª x å½’åˆ°1è¿™ä¸€ç±»ï¼Œå¦‚æœ y<0.5 å°±å°† x å½’åˆ°0è¿™ä¸€ç±»ã€‚

ä½†æ˜¯é˜ˆå€¼æ˜¯å¯ä»¥è°ƒæ•´çš„ï¼Œæ¯”å¦‚è¯´ä¸€ä¸ªæ¯”è¾ƒä¿å®ˆçš„äººï¼Œå¯èƒ½å°†é˜ˆå€¼è®¾ä¸º0.9ï¼Œä¹Ÿå°±æ˜¯è¯´æœ‰è¶…è¿‡90%çš„æŠŠæ¡ï¼Œæ‰ç›¸ä¿¡è¿™ä¸ª x å±äº1è¿™ä¸€ç±»ã€‚

::: info å¯¹æ¯”é€»è¾‘å›å½’ä¸çº¿æ€§å›å½’

- Linear Regression: $-\infty<h_{\theta}(x)<+\infty, \quad h_{\theta}(x)=\theta_{0} x_{0}+\theta_{1} x_{1}+\cdots+\theta_{n} x_{n}=\theta^{T} x$
- Logistic Regression:  $0 \leq h_{\theta}(x) \leq 1, \quad h_{\theta}(x)=g\left(\theta^{T} x\right)=\frac{1}{1+e^{-\theta^{T} x}}$

:::

## Representation

- $h_{\theta}(x)$ represents the estimated **probability** that ğ‘¦ = 1 on input ğ‘¥
- $h_{\theta}(x)$ = ğ‘ƒ{ğ‘¦=1|ğ‘¥,ğœƒ} means probability of ğ‘¦ = 1, given ğ‘¥, under parameter values ğœƒ
- $P\{y=0 \mid x, \theta\}=1-h_{\theta}(x)$

<img src="https://pic.hanjiaming.com.cn/2023/02/01/48dbf6a97250e.png" alt="CleanShot 2023-02-01 at 16.35.03@2x.png" style="zoom:33%;" />

## Decision boundary

$\theta^{T} x=0$ is the decision boundary

Example: 

- Assume: $h_{\theta}(x)=g\left(-3+x_{1}+x_{2}\right)$
- Decision boundary: $-3+x_{1}+x_{2}=0$
- Predict y = 1 when $-3+x_{1}+x_{2} \geqslant 0$
  - $x_{1}+x_{2} \geqslant 3$  (red)

<img src="https://pic.hanjiaming.com.cn/2022/07/11/e9372f8b677d5.png" alt="1657469988121.png" style="zoom: 50%;" />

## Regression Metrics

- MSE: Mean Square Error: $\frac{1}{n} \sum_{i=1}^{n}\left(\hat{y}_{t}-y_{t}\right)^{2}$
- MAE: Mean Absolute Error: $\frac{1}{n} \sum_{i=1}^{n}\left|\hat{y}_{t}-y_{t}\right|$
- MAPE: Mean Absolute Percentage Error: $\frac{100 \%}{n} \sum_{i=1}^{n}\left|\frac{\hat{y}_{t}-y_{t}}{y_{t}}\right|$

## Cost Function

é€»è¾‘å›å½’ä¸€èˆ¬ä½¿ç”¨äº¤å‰ç†µä½œä¸ºä»£ä»·å‡½æ•°ã€‚æ ¹æ®é€»è¾‘å›å½’æ¨¡å‹çš„ä»£ä»·å‡½æ•°ä»¥åŠ sigmoidå‡½æ•°
$$
g(z)=\frac{1}{1+e^{-z}}, \quad h_{\theta}(x)=g\left(\theta^{T} x\right)
$$

- å¯¹ä¸€ä¸ªæ ·æœ¬çš„costï¼Œæœ‰ $\operatorname{Cost}\left(h_{\theta}(x), y\right)=\left\{\begin{array}{cc}
  -\log \left(h_{\theta}(x)\right) & y=1 \\
  -\log \left(1-h_{\theta}(x)\right) & y=0
  \end{array}\right.$
- æ•´ä½“çš„ Cost Function ä¸º $J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \operatorname{Cost}\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right)$

è¿™æ ·åšæ˜¯ä¸ºäº† **Heavy Penalty**ï¼Œ å³

<img src="https://pic.hanjiaming.com.cn/2023/02/01/b1b624649b91a.png" alt="1675262460266.png" style="zoom: 25%;" />

æ³¨ï¼šåªè§‚å¯Ÿ [0,1] åŒºé—´

Cost Function:
$$
\operatorname{Cost}\left(h_{\theta}(x), y\right)=-y \log h_{\theta}(x)-(1-y) \log \left(1-h_{\theta}(x)\right)
$$
åŒ–ç®€ä¹‹åä¸º
$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m}\left(y^{(i)} \log h_{\theta}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]\right.
$$

- è®­ç»ƒæ ·æœ¬çš„ä¸ªæ•°ï¼›
- h<sub>Î¸</sub>(x)ï¼šç”¨å‚æ•°Î¸å’Œxé¢„æµ‹å‡ºæ¥çš„yå€¼ï¼›
- yï¼šåŸè®­ç»ƒæ ·æœ¬ä¸­çš„yå€¼ï¼Œä¹Ÿå°±æ˜¯æ ‡å‡†ç­”æ¡ˆ
- ä¸Šè§’æ ‡(i)ï¼šç¬¬iä¸ªæ ·æœ¬

$x^{(i)}$åå¯¼æ•°ä¸º
$$
\frac{\partial J\left(\theta_{0}, \theta_{1}, \cdots\right)}{\partial \theta_{j}}=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
$$
å’Œçº¿æ€§å›å½’ä¸€æ ·ï¼Œ
$$
\theta_{j}=\theta_{j}-\alpha \frac{\partial J\left(\theta_{0}, \theta_{1}, \cdots\right)}{\partial \theta_{j}}
$$
::: tip å¸¦æœ‰æ­£åˆ™åŒ–çš„ä»£ä»·å‡½æ•°
$$
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}
$$
:::

::: details ä¸ºä½•é€»è¾‘å›å½’è¦é‡æ–°å®šä¹‰ä»£ä»·å‡½æ•°

å¦‚æœåœ¨é€»è¾‘å›å½’ä¸­ä½¿ç”¨ cost function: $J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \frac{1}{2}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$ ï¼Œå…¶å›¾åƒä¸º

![1657472131550.png](https://pic.hanjiaming.com.cn/2022/07/11/9a62cf1a855f0.png)

è¿™æ˜¯ä¸€ä¸ªéå‡¸å‡½æ•°ï¼Œæœ‰å¤šä¸ªå±€éƒ¨æœ€ä¼˜è§£ï¼Œè¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•å¹¶ä¸ä¼šæ”¶æ•›åˆ°å®ƒçš„å…¨å±€æœ€ä¼˜è§£ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦é‡æ–°å®šä¹‰ä»£ä»·å‡½æ•°ã€‚

å½“ y = 1 æ—¶ï¼ŒCost(h<sub>Î¸</sub>(x), y) å‡½æ•°çš„å›¾åƒæ˜¯

<img src="https://pic.hanjiaming.com.cn/2022/07/11/67f362d682794.png" alt="1657474758955.png" style="zoom:50%;" />

å½“ y = 0æ—¶ï¼ŒCost(h<sub>Î¸</sub>(x), y) å‡½æ•°çš„å›¾åƒæ˜¯

<img src="https://pic.hanjiaming.com.cn/2022/07/11/99b2bc1475c75.png" alt="1657474805716.png" style="zoom:50%;" />

- é¢„æµ‹æ­£ç¡®ï¼Œé‚£ä¹ˆå®ƒçš„ä»£ä»·å€¼å°±ä¸º 0
- é¢„æµ‹é”™è¯¯ï¼Œé‚£ä¹ˆè¿™ä¸ªä»£ä»·å°±æ˜¯æ— ç©·å¤§çš„

:::

## Multi-class Classification

![1675263154245.png](https://pic.hanjiaming.com.cn/2023/02/01/d2ffce0f64689.png)

åœ¨ One-vs-all (one-vs-rest) æ–¹æ³•ä¸­ï¼Œä¸ºæ¯ä¸ªç±»åˆ«è®­ç»ƒä¸€ä¸ªäºŒå…ƒåˆ†ç±»å™¨ï¼Œå°†è¯¥ç±»åˆ«ä¸æ‰€æœ‰å…¶ä»–ç±»åˆ«åŒºåˆ†å¼€æ¥ã€‚

ä¾‹å¦‚ï¼Œå‡è®¾æœ‰ä¸‰ä¸ªç±»ï¼ŒAã€Bã€Cã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†è®­ç»ƒä¸‰ä¸ªäºŒå…ƒåˆ†ç±»å™¨ï¼šä¸€ä¸ªç”¨äºAç±»ä¸å…¶ä»–ç±»çš„å¯¹æ¯”ï¼Œä¸€ä¸ªç”¨äºBç±»ä¸å…¶ä»–ç±»çš„å¯¹æ¯”ï¼Œä¸€ä¸ªç”¨äºCç±»ä¸å…¶ä»–ç±»çš„å¯¹æ¯”ã€‚

<img src="https://pic.hanjiaming.com.cn/2023/02/21/305ae3908fc6d.png" alt="1676950789065.png" style="zoom:33%;" />

```python
import numpy as np
from sklearn.linear_model import LogisticRegression

# create a toy dataset with three classes
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])
y = np.array([0, 1, 2, 1, 0, 2])

# create three binary classifiers, one for each class
classifiers = []
for i in range(3):
    y_binary = (y == i).astype(int)
    clf = LogisticRegression()
    clf.fit(X, y_binary)
    classifiers.append(clf)

# predict the class of a new instance
new_instance = np.array([[2.5, 3.5]])
scores = [clf.predict_proba(new_instance)[0][1] for clf in classifiers]
predicted_class = np.argmax(scores)

# Note that the argmax function returns the index of the highest score, which corresponds to the original class label in this example.
print("Predicted class:", predicted_class)
```

